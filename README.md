# Knowledge Distillation of Large Language Models for Bio-species Information Extraction. (still  under development...........)

[![arXiv](https://img.shields.io/badge/arXiv-2403.15430-B31B1B.svg)](https://arxiv.org/abs/2403.15430)

<p align="center">
  <img src="/images/GPT4species.png" width="60%" alt="Results">
</p>

<h3>Overview</h3>
Large language models (LLM) are known for their vast knowledge coverage. But how much of that knowledge is relevant or accurate for extracting specific information regarding biological species such as amphibians, arthropods, birds, fishes, etc.? In this project, we set out to answer this question. 

We leverage a state-of-the-art LLM (i.e., GPT-4) and distill its knowledge to create datasets for both named entity recognition (NER) and relation extraction (RE).

<h3>Prompting</h3>
To generate RE and NER data from GPT4, we utilised the prompt below:

```
A habitat provides the necessary resources for survival, such as food, water, shelter, and space
for breeding and movement for any particular plant, animal, or organism.

Let us define a new variable, i.e, species ==  "Aetobatus flagellum". 
Where does species live, what does species feed on and how does species breed? 
Provide the answer as a tuple in this format: [species live in....; species feed on ....; species breed by...] 
The answers inside the tuple should be as specific as possible. 
We want to develop an NER dataset which contains 4 new entities namely; SPECIES, HABITAT, FEEDING, BREEDING.
Specifically, for each species, create a new NER entity called “SPECIES”. 
After the phrase "live in" in the answer, add a new NER entity called "HABITAT" to indicate where the species reside. 
After the phrase "feed on" in the answer, create a new NER entity called "FEEDING". 
Finally, after the phrase "breed by" let us add the NER entity called "BREEDING" to indicate how the species reproduces.
At the time of printing out the answer, first provide the answer about the species defined above in natural language. 
Include any other names used for species. 
Then, provide the answer in form of two tuples. 
Tuple-1 contains only the answer; and Tuple-2 contains only BIO tags corresponding to each of the words in Tuple-1.
Print out Tuple-1 and Tuple-2. Skip one line. 
Last, print out all the words inside Tuple-1 and the BIO tag corresponding to all words in Tuple-1;
as tab-separated values in one table.
```

<h3>The Dataset(s)</h3>
Two separate datasets, one for NER and another for RE were created. Each consists of 1.8K sentences. 
Please find the two datasets at the following locations: 

```bash
.
├── NER_data
│   └── NER_Biospecies_data.txt
└── RE_data
    ├── raw_RE_GPT4_sentences.txt
    └── RE_Biospecies_data.txt
```
The file `NER_Biospecies_data.txt` contains the NER data in *CoNNL format* whereby `tokens` and `BIO tags` are `tab-separeted`.
On the other hand, `RE_Biospecies_data.txt` includes the RE data annotated with `markers` around the entities introduced in a relation. (More on this below, under RE data section!)
All the sentences generated by GPT4 (and verified by human annotators) are stored in `raw_RE_GPT4_sentences.txt`

<h4>NER Dataset</h4>
The tags are in **BIO** format. The named entities (NE) covered in this dataset are: SPECIES, HABITAT, FEEDING, BREEDING. 

Here is the list of BIO tags:
```
list_of_BIO_tags = { 'B-BREEDING', 'I-BREEDING', 'B-FEEDING', 'I-FEEDING', 'B-HABITAT', 'I-HABITAT', 'B-SPECIES', 'I-SPECIES', 'O'
                     }
```
And the format of the NER data is as follows:
***

```
{ 
'tokens': {"0": ["Smoothtooth", "blacktip", "shark", "or", "Carcharhinus", "leiodon", "live", "in", "warm", "coastal", "waters", "particularly", "in", "the", "Indo-Pacific", "region"], ...},
 'tags': {"0": ["B-SPECIE", "I-SPECIE", "I-SPECIE", "O", "B-SPECIE", "I-SPECIE", "O", "O", "O", "O", "O", "O', "O", "O", "O" "O"], ...}
 }
```
***
The two dictionaries `label2id` and `id2label` are shown below: 

Dictionary One: `label2id`
```
labels_to_ids = {
    'B-BREEDING': 0,
    'B-FEEDING': 1,
    'B-HABITAT': 2,
    'B-SPECIES': 3,
    'I-BREEDING': 4,
    'I-FEEDING': 5,
    'I-HABITAT': 6,
    'I-SPECIES': 7,
    'O': 8
}
```
Dictionary Two: `id2label`
```
ids_to_labels = {v: k for k, v in labels_to_ids.items()}
 ```
 ***
<h4>Relation Extraction Data</h4>
We deployed GPTP-4 to generate information about the species' habitats, feeding, and reproduction patterns. The three primary relation classes in our data include: `live_in`, `feed_on`, and `breed_by` classes. 

Here is our RE class to id dictionary:
```
RE_labels = {
    'live_in' : 0,
    'feed_on': 1,
    'breed_by': 2,
    'other': 3,
}
```

To develop a usable RE dataset, we apply the formatting introduced in the paper **Matching the Blanks: Distributional Similarity for Relation Learning**, whose link is [here](https://github.com/jpablou/Relation-Extraction-using-Matching-the-Blanks-methodology-via-Google-BERT-domain-adaptation).

Specifically, we introduce unique `markers` [E1] and [/E1] around the first entity, together with  [E2] and [/E2] around the second entity, in a sentence comprising one relation and two entities (E1 and E2).
For example, in the sentence "Holy mountain salamander feed on insects", the relation is "feed on" and the two entities are "Holy mountain salamander" (i.e., E1) and insects (i.e., E2)

RE dataset format is shown below:
```
[E1]Holy mountain salamander[/E1] feed on [E2]insects[/E2]
```
The number of instances per relation class is summarized below:
| Relation | Count |
|----------|-------|
| breed by | 570   |
| feed on  | 582   |
| live in  | 626   |

<h3>How correct were GPT4 generated sentences?</h3>
We measured the correctness of sentences generated by GPT4 using the F1-score, for each the major information regarding the *breeding, feeding habitat* of bio-species.
<p align="left">
  <img src="/images/GPT4correct.png" width="40%" alt="Results">
</p>

<h3>Experiments</h3>
We deploy several "small" LLMs available at HuggingFace for evaluation. These include BERT, BioBERT, and PubMedBERT.

Under the NER task, we fine-tuned the three LLMs. PubMedBERT performed better than both BERT and BioBERT.
<p align="left">
  <img src="/images/NER_f1scores.png" width="40%" alt="Results">
</p>

For the RE task, we finetuned one LLM i.e., BioBERT (specifically biobert-base-cased-v1.1) from HuggingFace. The weighted f1-score across relation classes is 92.3%. 

<h3>Is GPT-4 a good teacher?</h3>
It is essential to establish the effectiveness of GPT4 for distilling knowledge about bio species. We compared GPT4 and UniversalNER-7 B's abilities to extract species information from both `easy` and `hard` examples. We found that GPT4 is a better teacher than UniversalNER-7B, validating our strategy to use GPT4 for distilling species information.  

### Citation

If you find this work or dataset helpful in your research, please cite it as follows:

```bibtex
@misc{atuhurra2024distillingnamedentityrecognition,
  title         = {Distilling Named Entity Recognition Models for Endangered Species from Large Language Models}, 
  author        = {Atuhurra, Jesse and Dujohn, Seiveright Cargill and Kamigaito, Hidetaka and Shindo, Hiroyuki and Watanabe, Taro},
  year          = {2024},
  eprint        = {2403.15430},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2403.15430}
}

